{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable tensorflow warning messages.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the entire Mishnah, in Hebrew and English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mishnah_path = r'/Users/lee/Judaism/Sefaria_Contest/sefaria_txt/Mishnah/'\n",
    "\n",
    "directories = [x[0] for x in os.walk(mishnah_path)]\n",
    "english = [i for i in directories if 'English' in i and not 'Commentary' in i] \n",
    "hebrew = [i for i in directories if 'Hebrew' in i and not 'Commentary' in i] \n",
    "\n",
    "mishnah_english = ''\n",
    "mishnah_hebrew = ''\n",
    "\n",
    "for direct in english:\n",
    "    mishnah = open(direct + '/clean.txt', 'rb').read().decode(encoding='utf-8')\n",
    "    mishnah_english += mishnah\n",
    "    \n",
    "for direct in hebrew:\n",
    "    mishnah = open(direct + '/clean.txt', 'rb').read().decode(encoding='utf-8')\n",
    "    mishnah_hebrew += mishnah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Mishnah: 1164429 characters\n"
     ]
    }
   ],
   "source": [
    "print('Length of Mishnah: {} characters'.format(len(mishnah_hebrew)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 unique Hebrew characters\n",
      "92 unique English characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(mishnah_hebrew))\n",
    "print ('{} unique Hebrew characters'.format(len(vocab)))\n",
    "\n",
    "english_vocab = sorted(set(mishnah_english))\n",
    "print ('{} unique English characters'.format(len(english_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', \"'\", '(', ')', ',', '.', ':', 'ְ', 'ֱ', 'ֲ', 'ִ', 'ֵ', 'ֶ', 'ַ', 'ָ', 'ֹ', 'ֻ', 'ּ', 'ׁ', 'ׂ', 'א', 'ב', 'ג', 'ד', 'ה', 'ו', 'ז', 'ח', 'ט', 'י', 'ך', 'כ', 'ל', 'ם', 'מ', 'ן', 'נ', 'ס', 'ע', 'ף', 'פ', 'ץ', 'צ', 'ק', 'ר', 'ש', 'ת']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '}', '¼', '¾', '–', '‘', '’', '“', '”', '…', '\\uf0b7']\n"
     ]
    }
   ],
   "source": [
    "print(english_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove vowels from the Mishnah?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = ['ְ', 'ֱ', 'ֲ', 'ִ', 'ֵ', 'ֶ', 'ַ', 'ָ', 'ֹ', 'ֻ', 'ּ', 'ׁ', 'ׂ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the text, and create mapping dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in mishnah_hebrew])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " \"'\": 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " ',': 5,\n",
       " '.': 6,\n",
       " ':': 7,\n",
       " 'ְ': 8,\n",
       " 'ֱ': 9,\n",
       " 'ֲ': 10,\n",
       " 'ִ': 11,\n",
       " 'ֵ': 12,\n",
       " 'ֶ': 13,\n",
       " 'ַ': 14,\n",
       " 'ָ': 15,\n",
       " 'ֹ': 16,\n",
       " 'ֻ': 17,\n",
       " 'ּ': 18,\n",
       " 'ׁ': 19,\n",
       " 'ׂ': 20,\n",
       " 'א': 21,\n",
       " 'ב': 22,\n",
       " 'ג': 23,\n",
       " 'ד': 24,\n",
       " 'ה': 25,\n",
       " 'ו': 26,\n",
       " 'ז': 27,\n",
       " 'ח': 28,\n",
       " 'ט': 29,\n",
       " 'י': 30,\n",
       " 'ך': 31,\n",
       " 'כ': 32,\n",
       " 'ל': 33,\n",
       " 'ם': 34,\n",
       " 'מ': 35,\n",
       " 'ן': 36,\n",
       " 'נ': 37,\n",
       " 'ס': 38,\n",
       " 'ע': 39,\n",
       " 'ף': 40,\n",
       " 'פ': 41,\n",
       " 'ץ': 42,\n",
       " 'צ': 43,\n",
       " 'ק': 44,\n",
       " 'ר': 45,\n",
       " 'ש': 46,\n",
       " 'ת': 47}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove vowels from the Mishnah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = np.arange(8, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the text up into training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "מ\n",
      "ֵ\n",
      "א\n",
      "ֵ\n",
      "י\n"
     ]
    }
   ],
   "source": [
    "seq_length = 400\n",
    "examples_per_epoch = len(mishnah_hebrew)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (401,), types: tf.int64>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'מֵאֵימָתַי קוֹרִין אֶת שְׁמַע בְּעַרְבִית. מִשָּׁעָה שֶׁהַכֹּהֲנִים נִכְנָסִים לֶאֱכֹל בִּתְרוּמָתָן, עַד סוֹף הָאַשְׁמוּרָה הָרִאשׁוֹנָה, דִּבְרֵי רַבִּי אֱלִיעֶזֶר. וַחֲכָמִים אוֹמְרִים, עַד חֲצוֹת. רַבָּן גַּמְלִיאֵל אוֹמֵר, עַד שֶׁיַּעֲלֶה עַמּוּד הַשָּׁחַר. מַעֲשֶׂה שֶׁבָּאוּ בָנָיו מִבֵּית הַמִּשְׁתֶּה, אָמְרוּ לוֹ, לֹא קָרִינוּ אֶת שְׁמַע. אָמַר לָהֶם, אִם לֹא עָלָה עַמּוּד הַשַּׁחַר, חַיָּב'\n",
      "'ִין אַתֶּם לִקְרוֹת. וְלֹא זוֹ בִּלְבַד, אֶלָּא כָּל מַה שֶּׁאָמְרוּ חֲכָמִים עַד חֲצוֹת, מִצְוָתָן עַד שֶׁיַּעֲלֶה עַמּוּד הַשָּׁחַר. הֶקְטֵר חֲלָבִים וְאֵבָרִים, מִצְוָתָן עַד שֶׁיַּעֲלֶה עַמּוּד הַשָּׁחַר. וְכָל הַנֶּאֱכָלִים לְיוֹם אֶחָד, מִצְוָתָן עַד שֶׁיַּעֲלֶה עַמּוּד הַשָּׁחַר. אִם כֵּן, לָמָּה אָמְרוּ חֲכָמִים עַד חֲצוֹת, כְּדֵי לְהַרְחִיק אֶת הָאָדָם מִן הָעֲבֵרָה: \\nמֵאֵימָתַי קוֹרִין אֶ'\n",
      "'ת שְׁמַע בְּשַׁחֲרִית. מִשֶּׁיַּכִּיר בֵּין תְּכֵלֶת לְלָבָן. רַבִּי אֱלִיעֶזֶר אוֹמֵר, בֵּין תְּכֵלֶת לְכַרְתִּי. וְגוֹמְרָהּ עַד הָנֵץ הַחַמָּה. רַבִּי יְהוֹשֻׁעַ אוֹמֵר, עַד שָׁלֹשׁ שָׁעוֹת, שֶׁכֵּן דֶּרֶךְ בְּנֵי מְלָכִים לַעֲמֹד בְּשָׁלֹשׁ שָׁעוֹת. הַקּוֹרֵא מִכָּאן וְאֵילָךְ לֹא הִפְסִיד, כְּאָדָם הַקּוֹרֵא בַתּוֹרָה: \\nבֵּית שַׁמַּאי אוֹמְרִים, בָּעֶרֶב כָּל אָדָם יַטּוּ וְיִקְרְאוּ, וּבַבֹּק'\n",
      "'ֶר יַעַמְדוּ, שֶׁנֶּאֱמַר (דברים ו) וּבְשָׁכְבְּךָ וּבְקוּמֶךָ. וּבֵית הִלֵּל אוֹמְרִים, כָּל אָדָם קוֹרֵא כְדַרְכּוֹ, שֶׁנֶּאֱמַר (שם) וּבְלֶכְתְּךָ בַדֶּרֶךְ. אִם כֵּן, לָמָּה נֶאֱמַר וּבְשָׁכְבְּךָ וּבְקוּמֶךָ, בְּשָׁעָה שֶׁבְּנֵי אָדָם שׁוֹכְבִים, וּבְשָׁעָה שֶׁבְּנֵי אָדָם עוֹמְדִים. אָמַר רַבִּי טַרְפוֹן, אֲנִי הָיִיתִי בָא בַדֶּרֶךְ, וְהִטֵּתִי לִקְרוֹת, כְּדִבְרֵי בֵית שַׁמַּאי, וְסִכַּנְתּ'\n",
      "'ִי בְעַצְמִי מִפְּנֵי הַלִּסְטִים. אָמְרוּ לוֹ, כְּדַי הָיִיתָ לָחוּב בְּעַצְמְךָ, שֶׁעָבַרְתָּ עַל דִּבְרֵי בֵית הִלֵּל: \\nבַּשַּׁחַר מְבָרֵךְ שְׁתַּיִם לְפָנֶיהָ וְאַחַת לְאַחֲרֶיהָ, וּבָעֶרֶב שְׁתַּיִם לְפָנֶיהָ וּשְׁתַּיִם לְאַחֲרֶיהָ. אַחַת אֲרֻכָּה וְאַחַת קְצָרָה. מָקוֹם שֶׁאָמְרוּ לְהַאֲרִיךְ, אֵינוֹ רַשַּׁאי לְקַצֵּר. לְקַצֵּר, אֵינוֹ רַשַּׁאי לְהַאֲרִיךְ. לַחְתֹּם, אֵינוֹ רַשַּׁאי שֶׁלֹּא '\n"
     ]
    }
   ],
   "source": [
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to map from input text to target text (just the input text shifted to the right by one character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'מֵאֵימָתַי קוֹרִין אֶת שְׁמַע בְּעַרְבִית. מִשָּׁעָה שֶׁהַכֹּהֲנִים נִכְנָסִים לֶאֱכֹל בִּתְרוּמָתָן, עַד סוֹף הָאַשְׁמוּרָה הָרִאשׁוֹנָה, דִּבְרֵי רַבִּי אֱלִיעֶזֶר. וַחֲכָמִים אוֹמְרִים, עַד חֲצוֹת. רַבָּן גַּמְלִיאֵל אוֹמֵר, עַד שֶׁיַּעֲלֶה עַמּוּד הַשָּׁחַר. מַעֲשֶׂה שֶׁבָּאוּ בָנָיו מִבֵּית הַמִּשְׁתֶּה, אָמְרוּ לוֹ, לֹא קָרִינוּ אֶת שְׁמַע. אָמַר לָהֶם, אִם לֹא עָלָה עַמּוּד הַשַּׁחַר, חַיָּ'\n",
      "Target data: 'ֵאֵימָתַי קוֹרִין אֶת שְׁמַע בְּעַרְבִית. מִשָּׁעָה שֶׁהַכֹּהֲנִים נִכְנָסִים לֶאֱכֹל בִּתְרוּמָתָן, עַד סוֹף הָאַשְׁמוּרָה הָרִאשׁוֹנָה, דִּבְרֵי רַבִּי אֱלִיעֶזֶר. וַחֲכָמִים אוֹמְרִים, עַד חֲצוֹת. רַבָּן גַּמְלִיאֵל אוֹמֵר, עַד שֶׁיַּעֲלֶה עַמּוּד הַשָּׁחַר. מַעֲשֶׂה שֶׁבָּאוּ בָנָיו מִבֵּית הַמִּשְׁתֶּה, אָמְרוּ לוֹ, לֹא קָרִינוּ אֶת שְׁמַע. אָמַר לָהֶם, אִם לֹא עָלָה עַמּוּד הַשַּׁחַר, חַיָּב'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 35 ('מ')\n",
      "  expected output: 12 ('ֵ')\n",
      "Step    1\n",
      "  input: 12 ('ֵ')\n",
      "  expected output: 21 ('א')\n",
      "Step    2\n",
      "  input: 21 ('א')\n",
      "  expected output: 12 ('ֵ')\n",
      "Step    3\n",
      "  input: 12 ('ֵ')\n",
      "  expected output: 30 ('י')\n",
      "Step    4\n",
      "  input: 30 ('י')\n",
      "  expected output: 35 ('מ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 400), (64, 400)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                 batch_input_shape=[batch_size, None]),\n",
    "                                 tf.keras.layers.GRU(rnn_units,\n",
    "                                                     return_sequences=True,\n",
    "                                                     stateful=True,\n",
    "                                                     recurrent_initializer='glorot_uniform'),\n",
    "                                 tf.keras.layers.Dense(vocab_size)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size = len(vocab),\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units,\n",
    "                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           12288     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 48)            49200     \n",
      "=================================================================\n",
      "Total params: 3,999,792\n",
      "Trainable params: 3,999,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict characters with untrained RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 400, 48) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  8,  7, 47, 42, 16, 15,  2, 11, 46, 17, 23, 31,  3, 23, 14,  5,\n",
       "       31, 30, 22, 21, 27, 36, 11, 37, 42,  9,  2, 19,  4, 24, 44,  1,  5,\n",
       "       31, 41,  3, 12, 41, 38, 21, 34, 30, 38, 39, 11, 30, 26,  3, 24, 11,\n",
       "        1, 43, 11,  7,  0, 46, 19, 10, 15, 34,  4, 23,  2, 34, 47, 32,  4,\n",
       "       37, 11, 21,  7, 10,  9, 13, 36, 47, 17, 37, 15, 46, 43, 23, 15, 41,\n",
       "       10, 11, 41, 26, 31, 11, 16,  0, 30, 15, 45, 18, 36,  2, 24,  4, 41,\n",
       "       36, 13,  5, 25, 28, 44,  2, 14, 29, 32, 10, 27,  4, 34, 36, 35, 21,\n",
       "       34, 14, 36, 46,  4, 25, 13, 19, 17, 47, 30, 22, 43, 16, 10, 17,  7,\n",
       "       31, 40, 29, 13, 15,  6,  9, 41, 32, 27, 29,  9, 10, 28,  7, 17, 10,\n",
       "        4, 44, 36,  3, 45, 10, 22,  5,  1, 35,  4, 14, 34, 45,  6, 14, 14,\n",
       "        0, 36,  1,  3, 47, 39, 18, 37, 27,  9, 34, 37, 29, 26,  5, 16, 19,\n",
       "        3, 18, 39, 31, 36, 34,  4, 30, 38, 34, 25, 14,  9, 40,  8, 41, 14,\n",
       "        4, 40, 40, 46, 12, 40, 14, 43, 37, 34, 41, 40, 11, 10,  1, 32, 20,\n",
       "       15, 44,  8, 46, 30, 30, 34, 22, 19, 21, 37, 11, 33,  0, 26, 21, 46,\n",
       "       47, 24, 16, 41, 36, 45, 33,  4, 38, 35, 33, 39, 41,  6, 38,  7, 22,\n",
       "       40, 12, 27,  9, 40, 16, 24, 33, 40, 20,  1, 32, 32,  0, 38, 43, 21,\n",
       "       13, 25, 17, 17, 13, 17, 11, 39,  9, 26, 27, 11, 16, 22, 13, 20, 44,\n",
       "        3, 28, 16, 39, 38, 30, 10, 47, 17, 12, 16,  0, 27, 10,  4, 36, 25,\n",
       "       23, 43, 15, 44, 35, 45,  7, 14,  0, 22,  9, 17, 38,  7, 12,  7, 28,\n",
       "        4, 19, 32, 42, 10, 33, 17,  1, 44, 13, 37,  8, 43, 21, 32, 35, 13,\n",
       "       22, 26, 26, 41, 43, 28, 35, 16,  6, 23, 15, 37, 28, 29,  1, 40,  8,\n",
       "       47, 22, 46, 11,  9, 27, 44, 22, 23, 16, 21, 12, 11, 20,  6, 15, 38,\n",
       "       43, 38, 17, 27, 17, 40, 36,  1,  8, 26, 24, 32, 12, 43, 19, 46, 23,\n",
       "        2,  7, 14, 41,  0, 31, 24, 23, 22])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'ם לַעֲמֹד בְּשָׁלֹשׁ שָׁעוֹת. הַקּוֹרֵא מִכָּאן וְאֵילָךְ לֹא הִפְסִיד, כְּאָדָם הַקּוֹרֵא בַתּוֹרָה: \\nבֵּית שַׁמַּאי אוֹמְרִים, בָּעֶרֶב כָּל אָדָם יַטּוּ וְיִקְרְאוּ, וּבַבֹּקֶר יַעַמְדוּ, שֶׁנֶּאֱמ'\n",
      "\n",
      "Predictions: \n",
      " \"ֻזְֱִׂ\\nרם'ְֲ\\nט:דֻׂאחוו.חנ:'שזתלת,ֵמ,(חֹונףיָ(םהּׂ(םעןך(ס'שָאףךכ\\nֱפׁ\\nֶׁ.צרע'מַּׁאצטיכץעת,,ד.ַָ.ֲֲט)ְלַש ףְ.ְש,לְצָ..ָ ְםמףךאַבהמנכ(ֲטרֻץגכִ,ֻףֱז)ץצלֵ:ׁמץ.טְֵ,ף:ׁ\\n\\nֱזפה'זרּף.ָקאל.ב דקט:טשלּלמֲָ,נג:ֹץחוג\"\n"
     ]
    }
   ],
   "source": [
    "print('Input: \\n', repr(''.join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print('Predictions: \\n', repr(''.join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 400, 48)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       3.8713052\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "EPOCHS=20 #30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45/45 [==============================] - 157s 3s/step - loss: 3.2965\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 159s 4s/step - loss: 2.0513\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 159s 4s/step - loss: 1.6395\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 160s 4s/step - loss: 1.2376\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 160s 4s/step - loss: 0.7820\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 160s 4s/step - loss: 0.3774\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 160s 4s/step - loss: 0.1726\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 172s 4s/step - loss: 0.1146\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 178s 4s/step - loss: 0.0949\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 183s 4s/step - loss: 0.0853\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 184s 4s/step - loss: 0.0777\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 185s 4s/step - loss: 0.0733\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 185s 4s/step - loss: 0.0700\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 187s 4s/step - loss: 0.0670\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 189s 4s/step - loss: 0.0647\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 190s 4s/step - loss: 0.0627\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 189s 4s/step - loss: 0.0612\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 189s 4s/step - loss: 0.0595\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 191s 4s/step - loss: 0.0580\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 191s 4s/step - loss: 0.0569\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate=2000, temperature=1.0):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "    \n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in np.arange(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        \n",
    "    return start_string + ''.join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "fmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "fmodel.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "מֵאֵימָתַי קוֹרִין אֶת שְׁמַע בְּשַׁחֲרִית. מִשֶּׁיַּכִּיר בֵּין תְּכֵלֶת לְלָבָן. רַבִּי אֱלִיעֶזֶר אוֹמֵר, בָּהֶן חַיָּבִין. אֵלּוּ וָאֵלּוּ פְּטִיָּה אִם שָׁמֹעַ לְוַיֹּאמֶר, בֵּין וַיֹּאמֶר לֶאֱמֶת וְיַצִּיב. רַבִּי יְהוּדָה אוֹמֵר, בֵּין וַיֹּאמֶר לֶאֱמֶת וְיַצִּיב לֹא יַפְסִיק. אָמַר רַבִּי יְהוֹשֻׁעַ בֶּן קָרְחָה, לָמָּה קָדְמָה שְׁמַע לִוְהָיָה אִם שָׁמֹעַ, אֶלָּא כְדֵי שֶׁיְּקַבֵּל עָלָיו עֹל מַלְכוּת שָׁמַיִם תְּחִלָּה, וְאַחַר כָּךְ יְקַבֵּל עָלָיו עֹל מִצְוֹת. וְהָיָהָא הַשָּׁחַר. הֶקְטֵר חֲלָבִים וְאֵבָרִים, מִצְוָתָן עַד שֶׁיַּעֲלֶה עַמּוּד הַשָּׁחַר. אִם כֵּן, לָמָּה אָמְרוּ חֲכָמִים עַד חֲצוֹת, כְּדֵי לְהַרְחִיק אֶת הָאָדָם מִן הָעֲבֵרָה: \n",
      "מֵאֵימָתַי קוֹרִין אֶת שְׁמַע בֶּשְׁתֵּיוּ וַחֲכָלִים הַיּוֹם וְאַחַר כָּךְ מְבָרֵךְ עַל הַיַּיִן. וּבֵית הִלֵּל אוֹמְרִים, מְבָרֵךְ עַל הַיַּיִן \n",
      "חַיָּב אָדָם לְבָרֵךְ לְאַחֲרָיו, וְאֵינוֹ מְבָרֵךְ לְפָנָיו. רַבִּי יְהוּדָה אוֹמֵר, מְבָרֵךְ לִפְנֵיהֶם וּלְאַחֲרֵיהֶם: \n",
      "הָיָה עוֹמֵד בַּתְּפִלָּה, וְנִזְכַּר שֶׁהוּא בַעַל קְרִי, לֹא יַפְסִיק, אֶלָּא יְקַצֵּר. יָת לִבְבַּת זֶרַע, וְהַמְשַׁמֶּשֶׁת שֶׁרָאֲתָה נִדָּה, צְרִיכִין טְבִילָת, בְּנֵיָה אוֹמֵר, נְבָרֵךְ לַייָ אֱלֹהֵינוּ. בְּמֵאָה וְהוּא, אוֹמֵר בָּרְכוּ. בְּאֶלֶף, אוֹמֵר נְבָרֵךְ לַייָ אֱלֹהֵינוּ אֱלֹהֵי יִשְׂרָאֵל. בְּאֶלֶף וְהוּא, אוֹמֵר בָּרְכוּ. בְּרִבּוֹאכוֹ, יְיִי מָרוֹא, אֶי עַד פָּרוֹת רָאשֶׁה אוֹמֵר, אוֹמְרָהּ בְּרָכָה אַחַת מֵעֵין שָׁלשׁ. רַבִּי עֲקִיבָא אוֹמֵר, אֲפִלּוּ אָכַל שֶׁלֶק וְהוּא מְזוֹנוֹ, מְבָרֵךְ אַחֲרָיו שָׁלשׁ בְּרָכָּהַ יְכַוְּנִין עַל הַנֵּר עַד שֶׁיֵּאוֹתוּ לְאוֹרוֹ: \n",
      "מִי שֶׁאָכַל וְשָׁכַח וְלֹא בֵרַךְ, בֵּית שַׁמַּאי אוֹמְרִים, יַחֲזֹר לִמְקוֹמוֹ וִיבָרֵךְ. וּבֵית הִלֵּל אוֹמְרִים, יְבָרֵךְ בַּמָּקוֹם שֶׁנִּזְכָּר. עַד אֵימָתַי הוּא מְבָרֵךְ. עַד כְּדֵי שֶׁיִּתְעַכֵּל הַמָּזוֹן שֶׁבְּמֵעָיו: \n",
      "בָּא לָהֶם יַיִן לְאַחַר הַמָּזוֹן וְאֵין שָׁם אֶלָּא אוֹתוֹ הַכּוֹס, בֵּית שַׁמַּאי אוֹמְרִים, מְבָרֵךְ עַל הַיָּיִן. עוֹנִין אָמֵן אַחַר יִשְׂרָאֵל הַמְבָרֵךְ, וְאֵין עוֹנִין אָמֵן אַחַר הַכּוּתִי הַמְבָרֵךְ, עַד שֶׁיִּשְׁמַע כָּל הַבְּרָכָה: \n",
      "הָרוֹאֶה\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(fmodel, u'מֵאֵימָתַי'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "בְּרָבָּה וְאַחַר כָּךְ מְבָרֵךְ עַל הַיַּיִן. וּבֵית הִלֵּל אוֹמְרִים, מְבָרֵךְ עַל הַיַּיִן וְאַחַר כָּךְ מְבָרֵךְ עַל הַיּוֹם: \n",
      "בֵּית שַׁמַּאי אוֹמְרִים, נִפְנָס יִטֹּם: \n",
      "מִי שֶׁמֵּתוֹ וּלְאַחֲרֵיהֶם שָׁלשׁ בְּרָכוֹת, דִּבְרֵי רַבָּן גַּמְלִיאֵל. וַחֲכָמִים אוֹמְרִים, בְּרָכָה אַחַת מֵעֵין שָׁלשׁ. רַבִּי עֲקִיבָא אוֹמֵר, אֲפִלּוּ אָכַל שֶׁלֶק וְהוּא מְזוֹנוֹ, מְבָרֵךְ אַחֲרָיו שָׁלשׁ בְּרָכוֹת. הַשּׁוֹתֶה מַיִם לִצְמָאוֹ, אוֹמֵר שֶׁהַכֹּל נִהְיֶה בִּדְבָרוֹ. רַבִּי טַרְפוֹן אוֹמֵר, בּוֹרֵא נְפָשׁוֹת רַבּוֹת: \n",
      "שְׁלשָׁה שֶׁאָכְלוּ כְאֶחָד, חַיָּבִין לְזַמֵּן. אָכַל דְּמַאי, וּמַעֲשֵׂר רִאשׁוֹן שֶׁנִּטְּלָה תְרוּמָתוֹ, וּמַעֲשֵׂר שֵׁנִי וְהֶקְדֵּשׁ חֶלֶי עֲקוֹדָה זָרָה. אֵין מְבָרְכִין עַל הַנֵּר עַד שֶׁיֵּאוֹתוּ לְאוֹרוֹ: \n",
      "מִי שֶׁאָכַל וְשָׁכַח וְלֹא בֵרַךְ, בֵּית שַׁמַּאי אוֹמְרִים, יַחֲזֹר לִמְקוֹמוֹ וִיבָרֵךְ. וּבֵית הִלֵּל אוֹמְרִים, יְבָרֵךְ בַּמָּקוֹם שֶׁנִּזְכָּר. עַד אֵימָתַי הוּא מְבָרֵךְ. עַד כְּדֵי שֶׁיִּתְעַכֵּל הַמָּזוֹן שֶׁבְּמֵעָיו: \n",
      "בָּא לָהֶם יַיִן לְאַחַר הַמָּזוֹן וְאֵין שָׁם אֶלָּא אוֹתוֹ הַכּוֹס, בֵּית שַׁמַּאי אוֹמְרִים, מְבָרֵךְ עַל הַיּוֹם: \n",
      "בֵּית שַׁמַּאי אוֹמְרִים, נוֹטְלִין לַיָּדַיִם, וְאַחַר כָּךְ מוֹזְגִין אֶת הַכּוֹס וְאַחַר כָּךְ נוֹטְלִין לַיָּדָיִם: \n",
      "בֵּית שַׁמַּאי אוֹמְרִים, מְקַנֵּחַ יָדָיו בַּמַּפָּה וּמַנִּיחָהּ עַל הַשֻּׁלְחָן. וּבֵית הִלֵּל אוֹמְרִים, עַל הַכֶּסֶת: \n",
      "בֵּית שַׁמַּאי אוֹמְרִים, מְכַבְּדִין אֶת הַבַּיִת וְאַחַר כָּךְ נוֹטְלִין לַיָּדַיִם. וּבֵית הִלֵּל אוֹמְרִים, נוֹטְלִין לַיָּדַיִם וְאַחַר כָּךְ מְכַבְּדִין אֶת הַבָּיִת: \n",
      "בֵּית שַׁמַּאי אוֹמְרִים, נֵר וּמָזוֹן וּבְשָׂמִים וְהַבְדָּלָה. וּבֵית הִלֵּל אוֹמְרִים, נֵר וּבְשָׂמִים וּמָזוֹן וְהַבְדָּלָה. בֵּית שַׁמַּאי אוֹמְרִים, שֶׁבָּרָא מְאוֹר הָאֵשׁ. וּבֵית הִלֵּל אוֹמְרִים, נוֹטְלִין לַיָּדַיִתְבַּסֶּה וְיִקְרָא. וְאִם לָאו, יִתְכַּסֶּה בַמַּיִם וְיִקְרָא. אֲבָל לֹא יִתְכַּסֶּה, לֹא בַמַּיִם הָרָעִים וְלֹא בְמֵי הַמִּשְׁרָה, עַד שֶׁיַּטִּיל לְתוֹכָן מָיִם. וְכַמָּה יַרְחִיק מֵהֶם וּמִן הַצּוֹאָה, אַרְבַּע אַמּוֹת: \n",
      "זָב שֶׁרָאָה קְרִי, אֲנִי הָעֵץ, לֹא יָצָא. עַל כּ\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(fmodel, u'בְּ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "עַל הַבְּשָׂמִים שֶׁלִּפְנֵי עֲבוֹדָה זָרָה. אֵין מְבָרְכִין עָלָיו: \n",
      "הָיוּ לְפָנָיו מִינִים הַרְבֵּה, רַבֵּנוּ, שֶׁאָבֵל אָסוּר לִרְחֹץ. אָמַר לָהֶם, אֵינִי כִשְׁאָר כָּל אָדָם, אִססטְרֵא לְמַפְרֵעַ, לֹא יָצָא. קָרָא וְטָעָה, יַחֲזֹר לְמָקוֹם שֶׁטָּעָה: \n",
      "הָאֻמָּנִין קוֹרִין בְּרֹאשׁ הָאִילָן אוֹ בְּרוֹבָן אַי רָעו, לֹא יַפְסִיק: \n",
      "מַזְכִּירִין גְּבוּרוֹת גְּשָׁמִים בִּתְחִיַּת הַמֵּתִים, וְנוֹתֵן הוֹדָאָה: \n",
      "הָאוֹמֵר עַל קַן צִפּוֹר יַגִּיעוּ רַחֲמֶיךָ, וְעַל טוֹב יִזַּכּוּר הַרִּיבָן אִם רָקוֹת אוֹמֵר בָּרוּךְ דַּיַּן הָאֱמֶת: \n",
      "בָּנָה בַיִת חָדָשׁ, וְקָנָה כֵלִים חֲדָשִׁים, אוֹמֵר בָּרוּזֶה וְטוּ אֱלֹהִים, יְיָ מִמְּקוֹר יִשְׂרָאֵר (תהלים סח). אָמַר רַבִּי עֲקִיבָא, מַה מָּצְאָדָם נוֹ. כָּל אָדָם קוֹרֵא כְדַרְכּוֹ, שֶׁנֶּאֱמַר (שם) וּבְלֶכְתְּךָ בַדֶּרֶךְ. אִם כֵּן, לָמָּה נֶאֱמַר וּבְשָׁכְבְּךָ וּבְקוּמֶךָ, בְּשָׁעָה שֶׁבְּנֵי אָדָם שׁוֹכְבִים, וּבְשָׁעָה שֶׁבְּנֵי אָדָם עוֹמְדִים. אָמַר רַבִּי טַרְפוֹן, אֲנִי הָיִיהִי רָצוֹן שֶׁתֵּלֵד אִשְׁתִּי זָכָר, הֲרֵי זוֹ תְּפִלַּת שָׁוְא. הָיָה בָא בַדֶּרֶךְ וְשָׁמַע קוֹל צְוָחָה בָּעִיר, וְאָמַר יְהִי רָצוֹן שֶׁלֹּא יִהְיוּ אֵלּוּ בְּנֵי בֵיתִי, הֲרֵי זוֹ תְּפִלַּת שָׁוְא: \n",
      "הַנִּכְנָס לִכְרַךְ מִתְפַּלֵּל שְׁתַּיִם, אַחַת בִּכְנִיסָתוֹ וְאַחַת בִּיצִיאָתוֹ. בֶּן עַזַּאי אוֹמֵר, אַרְבַּע, שְׁתַּיִם בִּכְנִיסָתוֹ לְבֵא: \n",
      "בֵּית שַׁמַּאי אוֹמְרִים, נוֹטְלִין לַיָּדַיִם, וְאַחַר כָּךְ מוֹזְגִין אֶת הַכּוֹס. וּבֵית הִכֶּׁםלֹא נֶחָתָקִין אֶת הַכּוֹס וְאַחַר כָּךְ נוֹטְלִין לַיָּדָיִם: \n",
      "בֵּית שַׁמַּאי אוֹמְרִים, נוֹטְלִין לַיָּדַיִם, וְאַחַר כָּךְ מוֹזְגִין אֶת הַכּוֹס. וּבֵית הִלֵּל אוֹמְרִים, מוֹזְגִין אֶת הַכּוֹס וְאַחַר כָּךְ נוֹטְלִין לַיָּדָיִם: \n",
      "בֵּית שַׁמַּאי אוֹמְרִים, נֵר וּמָזוֹן וּבְשָׂמִים וְאַחַר כָּךְ מְבָרֵךְ עַל הַיּוֹם: \n",
      "בֵּית שַׁמַּאי אוֹמְרִים, נוֹטְלִין לַיַּדִּי חֲנִידָאוֹ וַּלְקוּמָתוֹ, וְלַע לוֹ. וְאִם שְׁלִיחַ צִבּוּר הוּא, סִיםָ לוֹ תַּלְמִידָיו, לֹא לִמַּדְתָּנוּ רַבֵּנוּ, שֶׁאֵין מְקַבְּלִין תַּנְחוּמִין עַל הָעֲבָדִים. אָמַר לָהֶם, אֵין טָבִי עַבְדִּי כִּשְׁאָר כָּל הָעֲבָדִים, כָּשֵׁר הָיָה: \n",
      "חָתָן אִם ר\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(fmodel, u'עַל'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
