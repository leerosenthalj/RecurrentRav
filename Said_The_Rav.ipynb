{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable tensorflow warning messages.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecurrentRav: A Recurrent Neural Network Taught to Write Pseudo-Mishnah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What's a Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network (RNN) is an algorithm that identifies patterns in sequences of numbers,  can use those patterns to predict the next number in a sequence, and therefore can generate sequences that satisfy learned patterns. By mapping the characters and letters of a language to unique numbers, we can use RNNs to generate synthetic texts with the style of some real text. For example, we could train a RNN on all of Shakespeare's plays, then use our RNN to compose pseudo-Shakespearean plays! Here's an example, taken from Andrej Karpathy's ['The Unreasonable Effectiveness of Recurrent Neural Networks'](http://karpathy.github.io/2015/05/21/rnn-effectiveness/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------\n",
    "\n",
    "PANDARUS: <br>\n",
    "Alas, I think he shall be come approached and the day <br>\n",
    "When little srain would be attain'd into being never fed, <br>\n",
    "And who is but a chain and subjects of his death, <br>\n",
    "I should not sleep. <br>\n",
    "\n",
    "Second Senator: <br>\n",
    "They are away this miseries, produced upon my soul, <br>\n",
    "Breaking and strongly should be buried, when I perish <br>\n",
    "The earth and thoughts of many states. <br>\n",
    "\n",
    "DUKE VINCENTIO: <br>\n",
    "Well, your wit is in the care of side and that. <br>\n",
    "\n",
    "Second Lord: <br>\n",
    "They would be ruled after this chamber, and <br>\n",
    "my fair nues begun out of the fact, to be conveyed, <br>\n",
    "Whose noble souls I'll have the heart of the wars. <br>\n",
    "\n",
    "Clown: <br>\n",
    "Come, sir, I will make did behold your worship.\n",
    "\n",
    "VIOLA: <br>\n",
    "I'll drink it.\n",
    "\n",
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed descriptions of how RNNs work, check out Andrej's blog, [Google's RNN Text Generation Tutorial](https://www.tensorflow.org/tutorials/text/text_generation) (from which I have drawn substantially for the work shown here), and many other online sources. In short, a RNN is a set of mathematical operators that, given some ordered set of values, predict the next value in that sequence. We can 'train' an RNN to imitate a given text by feeding the algorithm many N-character sequences of that text, including the character that immediately follows that sequence, and tweaking the RNN's mathematical operators so that it successfully predicts the N+1th character of each sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What does this have to do with the Mishnah?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing, at least not at first glance. However, the Mishnah can be treated as an ordered sequence of characters (letters, vowels, spaces, etc.) just like any other text. So let's train a recurrent neural network on the entire Mishnah, and see what we can create!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Let's write some fake Mishnah!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a neural net on the Mishnah, we first cleaned Sefaria's raw text files so that they can be uniformly converted from string characters into sequences of numbers. We performed this cleaning in the Clean_Texts notebook in this repo. We then transitioned into the Train_The_Rav notebook and converted the cleaned text into sequences of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we used Tensorflow, Google's machine learning library, to build a Recurrent Neural Network with a memory of 400 characters. In practice this means that our network has a memory of less than 200 Hebrew letters, since vowels, new lines, and spaces count as unique characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What can we learn from this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying to other texts (Talmud is tricky, ) <br>\n",
    "Studying the layers/patterns of RecurrentRav, with visualizations <br>\n",
    "Creating a live website demo, rather than a static notebook <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
